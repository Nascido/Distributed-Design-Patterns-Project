
O padrão **Circuit Breaker (Disjuntor)** é uma solução arquitetural crítica em sistemas distribuídos, cujo objetivo principal é aumentar a resiliência e a disponibilidade do sistema, prevenindo falhas em cascata que surgem de dependências externas lentas ou inoperantes.

### Explicação do Padrão Circuit Breaker

O Circuit Breaker atua como um _proxy_ que monitora as chamadas de serviço (requisições) a uma dependência externa. Ele lida com o problema de recursos críticos do cliente e do servidor ficarem ocupados, como _threads_ e memória, esperando repetidamente por _timeouts_ devido a problemas de conexão.

O padrão define três estados principais para gerenciar o fluxo de requisições:

#### 1. Closed (Fechado)

- **Comportamento:** Este é o estado operacional normal. As requisições são permitidas a passar para o serviço dependente.
- **Transição de Estado:** Se a taxa de sucesso for mantida, o Circuit Breaker permanece `Closed`. No entanto, se as falhas (erros ou _timeouts_) excederem um limite (threshold) predefinido, o disjuntor registra a falha e transiciona para o estado `Open`.

#### 2. Open (Aberto)

- **Comportamento:** Neste estado, o Circuit Breaker interrompe imediatamente as tentativas de chamadas ao serviço dependente e retorna um erro ao cliente. Ele não tenta rotear a requisição ao servidor externo.
- **Propósito:** O estado `Open` é crucial porque previne que o cliente continue gastando recursos valiosos (como _threads_) esperando por um serviço que já se sabe estar indisponível ou lento.
- **Transição de Estado:** O disjuntor permanece no estado `Open` por um período definido (_reset timeout_). Após a expiração deste tempo, o disjuntor transiciona automaticamente para o estado `Half-Open`.

#### 3. Half-Open (Semi-Aberto)

- **Comportamento:** Após o período de _timeout_ (definido no estado `Open`), o Circuit Breaker entra no estado `Half-Open` para testar se o serviço dependente se recuperou. Ele permite que um número limitado de requisições de teste passe para o serviço.
- **Transição de Estado:**
    - Se as requisições de teste forem bem-sucedidas, ele assume que o serviço foi restaurado e retorna ao estado `Closed`.
    - Se as requisições falharem novamente, ele transiciona imediatamente de volta para o estado `Open` e reinicia o período de _reset timeout_.

### Proteção ao Sistema de Cotações

No contexto do **Trabalho 2**, o Serviço de Cotações em Tempo Real depende de um serviço externo. O padrão Circuit Breaker é recomendado para este componente (`Serviço de Cotações em Tempo Real`) para resolver o desafio de proteger o serviço primário contra a falha ou lentidão do serviço externo.

A proteção é garantida da seguinte forma:

1. **Isolamento de Falhas:** Se o serviço externo começar a falhar consistentemente ou demorar a responder, o Circuit Breaker detecta que o limite de falhas foi alcançado e transiciona para o estado `Open`.
2. **Prevenção de Falhas em Cascata:** Uma vez em `Open`, o serviço de cotações para de enviar tráfego para a dependência falha, retornando imediatamente um erro ou um valor de _fallback_ para seus clientes internos. Isso impede que as _threads_ do Serviço de Cotações fiquem bloqueadas esperando por _timeouts_ longos, o que consumiria recursos e levaria o próprio Serviço de Cotações a falhar (falha em cascata).
3. **Gestão de Recursos Eficiente:** Ao evitar esperas repetitivas por _timeouts_, o Circuit Breaker garante uma gestão de recursos mais eficiente, permitindo que o Serviço de Cotações permaneça disponível para outras operações ou clientes que não dependem daquele serviço externo específico.


---


O padrão **Publisher/Subscriber (Pub/Sub)** é uma solução fundamental para a arquitetura de sistemas distribuídos que exige comunicação eficiente e desacoplada. É amplamente utilizado quando há a necessidade de difundir informações sobre eventos para múltiplos componentes que o emissor não conhece diretamente.

### Descrição do Padrão e Distribuição de Preços

No contexto de um Sistema de Cotações, o padrão Pub/Sub gerencia a **Distribuição Assíncrona e Desacoplada de Preços**.

1. **Publisher (Serviço de Cotações):** O Serviço de Cotações em Tempo Real atua como o _Publisher_, gerando novas cotações (eventos).
2. **Topics (Tópicos):** Cada nova cotação é encapsulada como uma mensagem associada a um _Tópico_, que pode ser específico para um ativo financeiro ou um tipo de evento.
3. **Subscriber (Clientes/Consumidores):** Componentes interessados em receber as atualizações, como clientes finais ou o Repositório de Dados Históricos, atuam como _Subscribers_. Eles se inscrevem nos _Topics_ que lhes interessam.

Quando o _Publisher_ envia a mensagem, ela é reliably stored (armazenada de forma confiável) pelo sistema de fila e subsequentemente entregue aos _Subscribers_ de maneira confiável.

### Responsabilidade do Componente Broker

O **Broker** é o intermediário central do sistema Pub/Sub. Sua função principal é gerenciar as mensagens e a difusão dos eventos, permitindo a comunicação assíncrona.

As responsabilidades críticas do Broker são:

1. **Gestão de Mensagens e Tópicos:** Receber as mensagens dos _Publishers_ e associá-las a um _Tópico_. O Broker gerencia essas mensagens, que são normalmente enfileiradas ou empilhadas.
2. **Armazenamento Confiável:** Armazenar as mensagens publicadas de forma confiável. Isso garante que as mensagens não sejam perdidas e possam ser entregues aos _Subscribers_.
3. **Distribuição:** O Broker é responsável por distribuir as mensagens aos _Subscribers_ inscritos naquele _Tópico_. A entrega pode ocorrer de duas formas: o _Subscriber_ solicita novas mensagens de um tópico (modelo pull), ou o Broker decide quando enviar (modelo push).

### Desacoplamento de Clientes e Gerador de Preços

O Pub/Sub atinge o desacoplamento porque o _Publisher_ (Serviço de Cotações) e os _Subscribers_ (Clientes) não precisam ter conhecimento explícito um do outro.

1. **Anonimato do Publisher:** O _Publisher_ simplesmente envia a nova cotação para o _Topic_ no Broker. O _Publisher_ não precisa saber quantos ou quais _Subscribers_ estão conectados ou gerenciando ativamente suas conexões.
2. **Flexibilidade do Consumidor:** Os _Subscribers_ podem ingressar ou sair da rede sem exigir modificações no _Publisher_. Isso é crucial em um sistema de cotações em tempo real com milhões de usuários, pois o serviço gerador de preços pode se concentrar apenas na geração e publicação eficiente, deixando a complexidade da distribuição e gestão de conectividade a cargo do Broker.

Desta forma, o padrão aumenta a **escalabilidade** e a **manutenibilidade**, pois diferentes componentes podem evoluir independentemente, contanto que mantenham o contrato de mensagens do _Topic_.


---

A aplicação do padrão **Particionamento (Sharding)** é fundamental para endereçar a escalabilidade e a latência do Repositório de Dados Históricos, um desafio arquitetônico chave do Sistema de Cotações. Este padrão permite que os dados sejam distribuídos por múltiplos servidores, mitigando os problemas de custo de armazenamento e alta carga de requisições de consulta em uma única máquina.

### Mecanismo de Divisão do Histórico de Transações

O Sharding envolve a divisão de um _storage layer_ em múltiplas partes disjuntas (_shards_), cada uma hospedada por uma máquina separada. No contexto do histórico de transações, isso é implementado através de uma **Função de Sharding** que determina o destino de cada registro:

		`  Shard=Função_de_Sharding(Chave_de_Requisição)  `

A arquitetura requer um nó de entrada, frequentemente chamado de **Nó Roteador (_Root Node_)** ou lógica de aplicação que, ao receber uma requisição de consulta ou persistência, examina a chave da requisição e a distribui para o _shard_ apropriado.

Para o histórico de transações, a chave pode ser o ID do ativo (se o objetivo for isolar dados por instrumento financeiro) ou um atributo sequencial, como o timestamp ou o ID da transação (se o objetivo for distribuir cronologicamente).

### Comparação das Estratégias de Particionamento

Existem diferentes mecanismos para definir a função de sharding e distribuir os dados. Para dados históricos, as abordagens por Intervalos e Dispersão (Hash) são as mais relevantes:

#### 1. Particionamento por Intervalo (Intervals)

Nesta estratégia, os dados são divididos com base em faixas (ranges) predefinidas da chave.

|Característica|Descrição|
|:--|:--|
|**Definição**|Os _shards_ são mapeados a intervalos contínuos de valores da chave de partição (ex: IDs de 1 a 1000 vão para o Shard 1).|
|**Vantagens**|Consultas baseadas em faixa (como "todas as transações entre data A e data B") são altamente eficientes, pois a consulta pode ser direcionada a um número limitado de _shards_. A adição de novos _shards_ (para extensões temporais futuras, por exemplo) é simples, sem exigir redistribuição de dados existentes.|
|**Desafios**|Pode levar à criação de **"Hot Shards"** (partições quentes) se a carga de trabalho ou a geração de dados não for uniforme. Por exemplo, se a chave de sharding for o timestamp e a maior parte do tráfego de leitura se concentrar nos dados mais recentes, o _shard_ correspondente estará sobrecarregado.|

#### 2. Particionamento por Hash (Dispersão)

O Particionamento por Hash utiliza uma função matemática, geralmente combinada com o operador Módulo (`%`), para mapear a chave do registro para o número do _shard_.

|Característica|Descrição|
|:--|:--|
|**Definição**|$\text{Shard} = \text{hash}(\text{chave}) \pmod N$ (onde $N$ é o número de _shards_). Este método garante que a distribuição dos dados seja uniforme.|
|**Vantagens**|Excelente uniformidade de distribuição e balanceamento de carga, assumindo que a função de _hash_ seja de alta qualidade. Consultas pontuais (como busca por ID) geralmente são rápidas.|
|**Desafios**|Aumentar ou diminuir o número de _shards_ ($N$) resulta na re-indexação da maioria das chaves, pois a função módulo muda drasticamente. Isso é conhecido como **"re-sharding"** e pode ser um processo complicado e custoso em termos de desempenho, especialmente em um _sharded cache_ ou banco de dados. Uma solução para este problema é o uso de _Consistent Hashing Functions_.|

### Recomendação de Implementação (Simplicidade para o Trabalho 2)

Para a implementação inicial do **Trabalho 2** (Sistema de Cotações), a estratégia de **Particionamento por Intervalo** é geralmente a mais simples e recomendada, especialmente se a chave de partição for temporal (timestamp ou data).

1. **Natureza Sequencial do Histórico:** Como estamos lidando com dados históricos de transações, que são inerentemente sequenciais, a divisão por intervalos de tempo ou faixas de IDs sequenciais é intuitiva.
2. **Facilidade de Adição de Shards:** Se for necessário adicionar capacidade futura, um novo _shard_ pode ser provisionado para armazenar dados a partir de uma data específica sem a necessidade de rebalancear (ou mover) todos os dados existentes nos _shards_ antigos.
3. **Implementação da Lógica de Roteamento:** A lógica para encontrar o servidor correto (Função de Sharding) torna-se uma simples verificação de limites (ex: `IF data_requerida >= Data_Inicial_Shard_X THEN usar Shard X`), o que é menos complexo do que implementar uma função de _hashing_ determinística e consistente que lide com a redistribuição dinâmica.

Portanto, para alcançar os objetivos de escalabilidade e latência do repositório de dados históricos com a maior simplicidade de implementação, o **Particionamento por Intervalo** baseado em uma chave sequencial (como o timestamp da transação) é a abordagem mais direta.